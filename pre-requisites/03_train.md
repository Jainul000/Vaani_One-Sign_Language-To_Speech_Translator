# `train.py` – LSTM Model Training Script

This script trains a **deep learning model** for sign language recognition using **pre-extracted MediaPipe landmarks**.

It consumes the `.npy` keypoint data generated by the landmark extraction script and produces a trained model capable of classifying sign gestures.

This script is used after **data recording** and **landmark extraction** in the larger Sign Language to Speech Conversion project.

## Purpose of the Script

This script is responsible for:

- Loading frame-wise landmark data from disk
- Assembling fixed-length gesture sequences
- Training an **LSTM-based classification model**
- Evaluating model performance on unseen data
- Saving the **trained model** and **preprocessing parameters**

It focuses entirely on **model training and evaluation**.

## Libraries Used

```python
import numpy as np
import os
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.regularizers import l2
from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau
from keras.optimizers import Adam
from sklearn.metrics import multilabel_confusion_matrix, accuracy_score
```

| Library | Role                      |
|---------|---------------------------|
| `scikit-learn`    | Dataset splitting and evaluation metrics |
| `numpy`   | Numerical operations and array handling       |
| `os`      | File and directory management |
| `keras`  | Model definition, training, and callbacks      |


## Configuration

```python
DATA_PATH = os.path.join('MP_Data')
actions = np.array(['buds', 'spray', 'grow'])
no_sequences = 30
sequence_length = 30
FEATURES_PER_FRAME = 225
```

### ⚠️ Important

These values must match the configuration used in:

- `record.py`
- Landmark extraction script

Any mismatch will cause incorrect training data alignment.

## Data Loading

### `load_data()`
```python
def load_data():
    sequences, labels = [], []
    label_map = {label: num for num, label in enumerate(actions)}

    for action in actions:
        for sequence in range(no_sequences):
            window = []
            for frame_num in range(sequence_length):
                try:
                    res = np.load(os.path.join(DATA_PATH, action, str(sequence), f"{frame_num}.npy"))
                    window.append(res)
                except:
                    window.append(np.zeros(FEATURES_PER_FRAME))
            sequences.append(window)
            labels.append(label_map[action])

    return np.array(sequences), to_categorical(labels).astype(int)
```

This function:

- Reads frame-wise `.npy` landmark files
- Groups frames into fixed-length sequences
- Converts action labels into numerical classes
- Pads missing frames with zeros to maintain consistency

Output shapes:

- `x`: (samples, sequence_length, FEATURES_PER_FRAME)
- `y`: (samples, number_of_actions)

## Model Architecture

### `create_model()`

```python
def create_model(input_shape, num_classes):
    model = Sequential([
        LSTM(128, return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.01)),
        BatchNormalization(),
        Dropout(0.3),

        LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01)),
        Dropout(0.3),

        LSTM(32),
        Dense(64, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model
```

Key points:

- Stacked LSTMs capture temporal gesture dynamics
- Regularization and dropout reduce overfitting
- Softmax output produces class probabilities

## Training Preparation

### Train/Test Split

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15
)
```

### Normalization

```python
mean, std = np.mean(X_train), np.std(X_train)
X_train = (X_train - mean) / (std + 1e-8)
X_test = (X_test - mean) / (std + 1e-8)
```

- Normalization improves convergence
- Mean and standard deviation are saved for inference consistency

## Callbacks

```python
callbacks = [
    TensorBoard(log_dir='Logs'),
    EarlyStopping(patience=50, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.1, patience=20, verbose=1)
]
```

These callbacks:

- Monitor training progress
- Prevent overtraining
- Adapt learning rate automatically

## Model Training

```python
history = model.fit(
    X_train, y_train,
    epochs=500,
    batch_size=32,
    validation_split=0.1,
    callbacks=callbacks,
    verbose=1
)
```

Training:

- Runs for up to 500 epochs
- Stops early if validation performance stops improving
- Uses a portion of training data for validation

## Evaluation

```python
y_pred = model.predict(X_test)
y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(y_pred, axis=1)
```
```python
print(multilabel_confusion_matrix(y_true, y_pred))
print(f"Accuracy: {accuracy_score(y_true, y_pred):.4f}")
```

Evaluation includes:

- Confusion matrix per class
- Overall classification accuracy

## Saving Outputs

```python
model.save('action.h5')
np.savez('preprocess_params.npz', mean=mean, std=std)
```

Saved files:

- `action.h5` – Trained LSTM model
- `preprocess_params.npz` – Normalization parameters

These are required for real-time inference and deployment.

## Summary

`train.py`:

- Trains an LSTM-based gesture recognition model
- Uses MediaPipe landmark features as input
- Produces a deployable classification model
- Forms the training stage of the Sign Language to Speech project

It is designed to be r**eliable**, **reproducible**, and **easy to modify**.
